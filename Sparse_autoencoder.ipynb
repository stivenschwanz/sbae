{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torchvision import datasets, transforms\n",
    "\n",
    "from utils import mnist, plot_graphs, plot_mnist # functions for loading and plotting MNIST\n",
    "import numpy as np\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# loads dataset\n",
    "\n",
    "mnist_transform = transforms.Compose([\n",
    "                transforms.ToTensor(),\n",
    "                transforms.Normalize((0.1307,), (0.3081,)),\n",
    "           ])\n",
    "train_loader, valid_loader, test_loader = mnist(valid=10000, transform=mnist_transform)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Encoder and decoder classes\n",
    "\n",
    "class Encoder(nn.Module):\n",
    "    def __init__(self, latent_size=10):\n",
    "        super(Encoder, self).__init__()\n",
    "        self.fc1 = nn.Linear(28*28, latent_size)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = torch.sigmoid(self.fc1(x))\n",
    "        return x\n",
    "    \n",
    "class Decoder(nn.Module):\n",
    "    def __init__(self, latent_size=10):\n",
    "        super(Decoder, self).__init__()\n",
    "        self.fc1 = nn.Linear(latent_size, 28*28)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = torch.tanh(self.fc1(x))\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Class for autoencoder\n",
    "\n",
    "class Net(nn.Module):\n",
    "    def __init__(self, latent_size=10, loss_fn=F.mse_loss, lr=1e-4, l2=0.):\n",
    "        super(Net, self).__init__()\n",
    "        self.latent_size = latent_size\n",
    "        self.E = Encoder(latent_size)\n",
    "        self.D = Decoder(latent_size)\n",
    "        self.loss_fn = loss_fn\n",
    "        self._rho_loss = None\n",
    "        self._loss = None\n",
    "        self.optim = optim.Adam(self.parameters(), lr=lr, weight_decay=l2)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = x.view(-1, 28*28)\n",
    "        h = self.E(x)\n",
    "        self.data_rho = h.mean(0) # calculates rho from encoder activations\n",
    "        out = self.D(h)\n",
    "        return out\n",
    "    \n",
    "    def decode(self, h):\n",
    "        with torch.no_grad():\n",
    "            return self.D(h)\n",
    "    \n",
    "    def rho_loss(self, rho, size_average=True):        \n",
    "        dkl = - rho * torch.log(self.data_rho) - (1-rho)*torch.log(1-self.data_rho) # calculates KL divergence\n",
    "        if size_average:\n",
    "            self._rho_loss = dkl.mean()\n",
    "        else:\n",
    "            self._rho_loss = dkl.sum()\n",
    "        return self._rho_loss\n",
    "    \n",
    "    def loss(self, x, target, **kwargs):\n",
    "        target = target.view(-1, 28*28)\n",
    "        self._loss = self.loss_fn(x, target, **kwargs)\n",
    "        return self._loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Making AEs with 16, 64 and 256 neurons in latent layer\n",
    "\n",
    "models = {\"16\": Net(16), \"64\": Net(64), \"256\": Net(256)}\n",
    "rho = 0.05\n",
    "train_log = {k: [] for k in models}\n",
    "test_log = {k: [] for k in models}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train function\n",
    "\n",
    "def train(epoch, models, log=None, add_noise=False, half_image=False):\n",
    "    train_size = len(train_loader.sampler)\n",
    "    for batch_idx, (data, _) in enumerate(train_loader):\n",
    "        for model in models.values():\n",
    "            model.optim.zero_grad()\n",
    "            inputs = data.clone().detach()\n",
    "            if add_noise:\n",
    "                inputs = noise_batch(inputs)\n",
    "            if half_image:\n",
    "                inputs = half_batch(inputs)\n",
    "            output = model(inputs)\n",
    "            rho_loss = model.rho_loss(rho)\n",
    "            loss = model.loss(output, data) + rho_loss\n",
    "            loss.backward()\n",
    "            model.optim.step()\n",
    "            \n",
    "        if batch_idx % 200 == 0:\n",
    "            line = \"Train Epoch: {} [{}/{} ({:.0f}%)]\\tLosses \".format(\n",
    "                epoch, batch_idx * len(data), train_size, 100. * batch_idx / len(train_loader))\n",
    "            losses = \" \".join([\"{}: {:.6f}\".format(k, m._loss.item()) for k, m in models.items()])\n",
    "            print(line + losses)\n",
    "            \n",
    "    else:\n",
    "        batch_idx += 1\n",
    "        line = \"Train Epoch: {} [{}/{} ({:.0f}%)]\\tLosses \".format(\n",
    "            epoch, batch_idx * len(data), train_size, 100. * batch_idx / len(train_loader))\n",
    "        losses = \" \".join([\"{}: {:.6f}\".format(k, m._loss.item()) for k, m in models.items()])\n",
    "        if log is not None:\n",
    "            for k in models:\n",
    "                log[k].append((models[k]._loss, models[k]._rho_loss))\n",
    "        print(line + losses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "avg_lambda = lambda l: \"loss: {:.4f}\".format(l)\n",
    "rho_lambda = lambda p: \"rho_loss: {:.4f}\".format(p)\n",
    "line = lambda i, l, p: \"{}: \".format(i) + avg_lambda(l) + \"\\t\" + rho_lambda(p)\n",
    "  \n",
    "# Test function    \n",
    "    \n",
    "def test(models, loader, log=None, add_noise=False, half_image=False):\n",
    "    test_size = len(loader.sampler)\n",
    "\n",
    "    test_loss = {k: 0. for k in models}\n",
    "    rho_loss = {k: 0. for k in models}\n",
    "    with torch.no_grad():\n",
    "        for data, _ in loader:\n",
    "            inputs = data.clone().detach()\n",
    "            if add_noise:\n",
    "                inputs = noise_batch(inputs)\n",
    "            if half_image:\n",
    "                inputs = half_batch(inputs)\n",
    "            output = {k: m(inputs) for k, m in models.items()}\n",
    "            for k, m in models.items():\n",
    "                test_loss[k] += m.loss(output[k], data, reduction=\"sum\").item()\n",
    "                rho_loss[k] += m.rho_loss(rho, size_average=False).item()\n",
    "    \n",
    "    for k in models:\n",
    "        test_loss[k] /= (test_size * 784)\n",
    "        rho_loss[k] /= (test_size * models[k].latent_size)\n",
    "        if log is not None:\n",
    "            log[k].append((test_loss[k], rho_loss[k]))\n",
    "    \n",
    "    lines = \"\\n\".join([line(k, test_loss[k], rho_loss[k]) for k in models]) + \"\\n\"\n",
    "    report = \"Test set:\\n\" + lines        \n",
    "    print(report)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 1 [0/50000 (0%)]\tLosses 16: 1.120975 64: 1.093070 256: 1.105797\n",
      "Train Epoch: 1 [10000/50000 (20%)]\tLosses 16: 0.983985 64: 0.919490 256: 0.740153\n",
      "Train Epoch: 1 [20000/50000 (40%)]\tLosses 16: 0.926673 64: 0.792052 256: 0.645233\n",
      "Train Epoch: 1 [30000/50000 (60%)]\tLosses 16: 0.988134 64: 0.787523 256: 0.656072\n",
      "Train Epoch: 1 [40000/50000 (80%)]\tLosses 16: 0.923780 64: 0.714846 256: 0.599620\n",
      "Train Epoch: 1 [50000/50000 (100%)]\tLosses 16: 0.866401 64: 0.667819 256: 0.561614\n",
      "Test set:\n",
      "16: loss: 0.8889\trho_loss: 0.0042\n",
      "64: loss: 0.6831\trho_loss: 0.0050\n",
      "256: loss: 0.5741\trho_loss: 0.0046\n",
      "\n",
      "Train Epoch: 2 [0/50000 (0%)]\tLosses 16: 0.910503 64: 0.695206 256: 0.588913\n",
      "Train Epoch: 2 [10000/50000 (20%)]\tLosses 16: 0.826106 64: 0.648865 256: 0.544066\n",
      "Train Epoch: 2 [20000/50000 (40%)]\tLosses 16: 0.822022 64: 0.652010 256: 0.554155\n",
      "Train Epoch: 2 [30000/50000 (60%)]\tLosses 16: 0.723279 64: 0.582757 256: 0.486319\n",
      "Train Epoch: 2 [40000/50000 (80%)]\tLosses 16: 0.798905 64: 0.641340 256: 0.539620\n",
      "Train Epoch: 2 [50000/50000 (100%)]\tLosses 16: 0.740768 64: 0.599605 256: 0.506789\n",
      "Test set:\n",
      "16: loss: 0.7382\trho_loss: 0.0048\n",
      "64: loss: 0.6012\trho_loss: 0.0047\n",
      "256: loss: 0.5034\trho_loss: 0.0045\n",
      "\n",
      "Train Epoch: 3 [0/50000 (0%)]\tLosses 16: 0.717778 64: 0.591977 256: 0.492989\n",
      "Train Epoch: 3 [10000/50000 (20%)]\tLosses 16: 0.729379 64: 0.602731 256: 0.503263\n",
      "Train Epoch: 3 [20000/50000 (40%)]\tLosses 16: 0.678435 64: 0.560649 256: 0.472034\n",
      "Train Epoch: 3 [30000/50000 (60%)]\tLosses 16: 0.699677 64: 0.573927 256: 0.474693\n",
      "Train Epoch: 3 [40000/50000 (80%)]\tLosses 16: 0.662484 64: 0.544841 256: 0.456565\n",
      "Train Epoch: 3 [50000/50000 (100%)]\tLosses 16: 0.687568 64: 0.562149 256: 0.474912\n",
      "Test set:\n",
      "16: loss: 0.6791\trho_loss: 0.0047\n",
      "64: loss: 0.5574\trho_loss: 0.0046\n",
      "256: loss: 0.4659\trho_loss: 0.0044\n",
      "\n",
      "Train Epoch: 4 [0/50000 (0%)]\tLosses 16: 0.684030 64: 0.564726 256: 0.471698\n",
      "Train Epoch: 4 [10000/50000 (20%)]\tLosses 16: 0.627310 64: 0.512621 256: 0.428509\n",
      "Train Epoch: 4 [20000/50000 (40%)]\tLosses 16: 0.672708 64: 0.546958 256: 0.455092\n",
      "Train Epoch: 4 [30000/50000 (60%)]\tLosses 16: 0.673553 64: 0.548910 256: 0.459717\n",
      "Train Epoch: 4 [40000/50000 (80%)]\tLosses 16: 0.645662 64: 0.524886 256: 0.437610\n",
      "Train Epoch: 4 [50000/50000 (100%)]\tLosses 16: 0.654943 64: 0.532021 256: 0.440329\n",
      "Test set:\n",
      "16: loss: 0.6468\trho_loss: 0.0046\n",
      "64: loss: 0.5290\trho_loss: 0.0045\n",
      "256: loss: 0.4418\trho_loss: 0.0043\n",
      "\n",
      "Train Epoch: 5 [0/50000 (0%)]\tLosses 16: 0.655828 64: 0.536622 256: 0.444649\n",
      "Train Epoch: 5 [10000/50000 (20%)]\tLosses 16: 0.631546 64: 0.515388 256: 0.425374\n",
      "Train Epoch: 5 [20000/50000 (40%)]\tLosses 16: 0.655107 64: 0.528869 256: 0.439786\n",
      "Train Epoch: 5 [30000/50000 (60%)]\tLosses 16: 0.703245 64: 0.577523 256: 0.487451\n",
      "Train Epoch: 5 [40000/50000 (80%)]\tLosses 16: 0.661560 64: 0.545856 256: 0.453904\n",
      "Train Epoch: 5 [50000/50000 (100%)]\tLosses 16: 0.634745 64: 0.530039 256: 0.444316\n",
      "Test set:\n",
      "16: loss: 0.6243\trho_loss: 0.0046\n",
      "64: loss: 0.5098\trho_loss: 0.0045\n",
      "256: loss: 0.4253\trho_loss: 0.0043\n",
      "\n",
      "Train Epoch: 6 [0/50000 (0%)]\tLosses 16: 0.617342 64: 0.502260 256: 0.418531\n",
      "Train Epoch: 6 [10000/50000 (20%)]\tLosses 16: 0.631504 64: 0.517328 256: 0.433955\n",
      "Train Epoch: 6 [20000/50000 (40%)]\tLosses 16: 0.641765 64: 0.528334 256: 0.440518\n",
      "Train Epoch: 6 [30000/50000 (60%)]\tLosses 16: 0.638209 64: 0.529279 256: 0.440730\n",
      "Train Epoch: 6 [40000/50000 (80%)]\tLosses 16: 0.617863 64: 0.505760 256: 0.419612\n",
      "Train Epoch: 6 [50000/50000 (100%)]\tLosses 16: 0.610339 64: 0.498322 256: 0.416999\n",
      "Test set:\n",
      "16: loss: 0.6076\trho_loss: 0.0045\n",
      "64: loss: 0.4961\trho_loss: 0.0044\n",
      "256: loss: 0.4142\trho_loss: 0.0043\n",
      "\n",
      "Train Epoch: 7 [0/50000 (0%)]\tLosses 16: 0.609091 64: 0.502283 256: 0.416759\n",
      "Train Epoch: 7 [10000/50000 (20%)]\tLosses 16: 0.599821 64: 0.487988 256: 0.402763\n",
      "Train Epoch: 7 [20000/50000 (40%)]\tLosses 16: 0.578112 64: 0.476297 256: 0.398438\n",
      "Train Epoch: 7 [30000/50000 (60%)]\tLosses 16: 0.589209 64: 0.478510 256: 0.396110\n",
      "Train Epoch: 7 [40000/50000 (80%)]\tLosses 16: 0.598257 64: 0.493439 256: 0.417169\n",
      "Train Epoch: 7 [50000/50000 (100%)]\tLosses 16: 0.649967 64: 0.525032 256: 0.441743\n",
      "Test set:\n",
      "16: loss: 0.5928\trho_loss: 0.0045\n",
      "64: loss: 0.4847\trho_loss: 0.0044\n",
      "256: loss: 0.4060\trho_loss: 0.0042\n",
      "\n",
      "Train Epoch: 8 [0/50000 (0%)]\tLosses 16: 0.634724 64: 0.527602 256: 0.441252\n",
      "Train Epoch: 8 [10000/50000 (20%)]\tLosses 16: 0.591867 64: 0.479057 256: 0.399956\n",
      "Train Epoch: 8 [20000/50000 (40%)]\tLosses 16: 0.590528 64: 0.482328 256: 0.403023\n",
      "Train Epoch: 8 [30000/50000 (60%)]\tLosses 16: 0.593055 64: 0.479200 256: 0.405524\n",
      "Train Epoch: 8 [40000/50000 (80%)]\tLosses 16: 0.581451 64: 0.468336 256: 0.391798\n",
      "Train Epoch: 8 [50000/50000 (100%)]\tLosses 16: 0.606802 64: 0.498039 256: 0.416746\n",
      "Test set:\n",
      "16: loss: 0.5826\trho_loss: 0.0044\n",
      "64: loss: 0.4757\trho_loss: 0.0044\n",
      "256: loss: 0.3994\trho_loss: 0.0042\n",
      "\n",
      "Train Epoch: 9 [0/50000 (0%)]\tLosses 16: 0.566611 64: 0.453735 256: 0.380133\n",
      "Train Epoch: 9 [10000/50000 (20%)]\tLosses 16: 0.580610 64: 0.482388 256: 0.405703\n",
      "Train Epoch: 9 [20000/50000 (40%)]\tLosses 16: 0.593971 64: 0.489007 256: 0.407689\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Input \u001b[1;32mIn [8]\u001b[0m, in \u001b[0;36m<cell line: 1>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m model \u001b[38;5;129;01min\u001b[39;00m models\u001b[38;5;241m.\u001b[39mvalues():\n\u001b[0;32m      3\u001b[0m     model\u001b[38;5;241m.\u001b[39mtrain()\n\u001b[1;32m----> 4\u001b[0m \u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mepoch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodels\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_log\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      5\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m model \u001b[38;5;129;01min\u001b[39;00m models\u001b[38;5;241m.\u001b[39mvalues():\n\u001b[0;32m      6\u001b[0m     model\u001b[38;5;241m.\u001b[39meval()\n",
      "Input \u001b[1;32mIn [6]\u001b[0m, in \u001b[0;36mtrain\u001b[1;34m(epoch, models, log, add_noise, half_image)\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mtrain\u001b[39m(epoch, models, log\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, add_noise\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m, half_image\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m):\n\u001b[0;32m      4\u001b[0m     train_size \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlen\u001b[39m(train_loader\u001b[38;5;241m.\u001b[39msampler)\n\u001b[1;32m----> 5\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m batch_idx, (data, _) \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(train_loader):\n\u001b[0;32m      6\u001b[0m         \u001b[38;5;28;01mfor\u001b[39;00m model \u001b[38;5;129;01min\u001b[39;00m models\u001b[38;5;241m.\u001b[39mvalues():\n\u001b[0;32m      7\u001b[0m             model\u001b[38;5;241m.\u001b[39moptim\u001b[38;5;241m.\u001b[39mzero_grad()\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\torch\\utils\\data\\dataloader.py:628\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    625\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sampler_iter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    626\u001b[0m     \u001b[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[0;32m    627\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reset()  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n\u001b[1;32m--> 628\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_next_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    629\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m    630\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset_kind \u001b[38;5;241m==\u001b[39m _DatasetKind\u001b[38;5;241m.\u001b[39mIterable \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[0;32m    631\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[0;32m    632\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m>\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called:\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\torch\\utils\\data\\dataloader.py:671\u001b[0m, in \u001b[0;36m_SingleProcessDataLoaderIter._next_data\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    669\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_next_data\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    670\u001b[0m     index \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_next_index()  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[1;32m--> 671\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_dataset_fetcher\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfetch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mindex\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[0;32m    672\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory:\n\u001b[0;32m    673\u001b[0m         data \u001b[38;5;241m=\u001b[39m _utils\u001b[38;5;241m.\u001b[39mpin_memory\u001b[38;5;241m.\u001b[39mpin_memory(data, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory_device)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\torch\\utils\\data\\_utils\\fetch.py:58\u001b[0m, in \u001b[0;36m_MapDatasetFetcher.fetch\u001b[1;34m(self, possibly_batched_index)\u001b[0m\n\u001b[0;32m     56\u001b[0m         data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset\u001b[38;5;241m.\u001b[39m__getitems__(possibly_batched_index)\n\u001b[0;32m     57\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m---> 58\u001b[0m         data \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[idx] \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m possibly_batched_index]\n\u001b[0;32m     59\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m     60\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[possibly_batched_index]\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\torch\\utils\\data\\_utils\\fetch.py:58\u001b[0m, in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m     56\u001b[0m         data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset\u001b[38;5;241m.\u001b[39m__getitems__(possibly_batched_index)\n\u001b[0;32m     57\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m---> 58\u001b[0m         data \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdataset\u001b[49m\u001b[43m[\u001b[49m\u001b[43midx\u001b[49m\u001b[43m]\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m possibly_batched_index]\n\u001b[0;32m     59\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m     60\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[possibly_batched_index]\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\torchvision\\datasets\\mnist.py:145\u001b[0m, in \u001b[0;36mMNIST.__getitem__\u001b[1;34m(self, index)\u001b[0m\n\u001b[0;32m    142\u001b[0m img \u001b[38;5;241m=\u001b[39m Image\u001b[38;5;241m.\u001b[39mfromarray(img\u001b[38;5;241m.\u001b[39mnumpy(), mode\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mL\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    144\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtransform \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m--> 145\u001b[0m     img \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtransform\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimg\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    147\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtarget_transform \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    148\u001b[0m     target \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtarget_transform(target)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\torchvision\\transforms\\transforms.py:95\u001b[0m, in \u001b[0;36mCompose.__call__\u001b[1;34m(self, img)\u001b[0m\n\u001b[0;32m     93\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m, img):\n\u001b[0;32m     94\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m t \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtransforms:\n\u001b[1;32m---> 95\u001b[0m         img \u001b[38;5;241m=\u001b[39m \u001b[43mt\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimg\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     96\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m img\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\torch\\nn\\modules\\module.py:1194\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1190\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1191\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1192\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1193\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1194\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39m\u001b[38;5;28minput\u001b[39m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1195\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1196\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\torchvision\\transforms\\transforms.py:270\u001b[0m, in \u001b[0;36mNormalize.forward\u001b[1;34m(self, tensor)\u001b[0m\n\u001b[0;32m    262\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, tensor: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[0;32m    263\u001b[0m     \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    264\u001b[0m \u001b[38;5;124;03m    Args:\u001b[39;00m\n\u001b[0;32m    265\u001b[0m \u001b[38;5;124;03m        tensor (Tensor): Tensor image to be normalized.\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    268\u001b[0m \u001b[38;5;124;03m        Tensor: Normalized Tensor image.\u001b[39;00m\n\u001b[0;32m    269\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 270\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnormalize\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtensor\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmean\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstd\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minplace\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\torchvision\\transforms\\functional.py:360\u001b[0m, in \u001b[0;36mnormalize\u001b[1;34m(tensor, mean, std, inplace)\u001b[0m\n\u001b[0;32m    357\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(tensor, torch\u001b[38;5;241m.\u001b[39mTensor):\n\u001b[0;32m    358\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mimg should be Tensor Image. Got \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mtype\u001b[39m(tensor)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m--> 360\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF_t\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnormalize\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtensor\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmean\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmean\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstd\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstd\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minplace\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minplace\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\torchvision\\transforms\\functional_tensor.py:929\u001b[0m, in \u001b[0;36mnormalize\u001b[1;34m(tensor, mean, std, inplace)\u001b[0m\n\u001b[0;32m    924\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m    925\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mExpected tensor to be a tensor image of size (..., C, H, W). Got tensor.size() = \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtensor\u001b[38;5;241m.\u001b[39msize()\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    926\u001b[0m     )\n\u001b[0;32m    928\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m inplace:\n\u001b[1;32m--> 929\u001b[0m     tensor \u001b[38;5;241m=\u001b[39m \u001b[43mtensor\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mclone\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    931\u001b[0m dtype \u001b[38;5;241m=\u001b[39m tensor\u001b[38;5;241m.\u001b[39mdtype\n\u001b[0;32m    932\u001b[0m mean \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mas_tensor(mean, dtype\u001b[38;5;241m=\u001b[39mdtype, device\u001b[38;5;241m=\u001b[39mtensor\u001b[38;5;241m.\u001b[39mdevice)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "for epoch in range(1, 51):\n",
    "    for model in models.values():\n",
    "        model.train()\n",
    "    train(epoch, models, train_log)\n",
    "    for model in models.values():\n",
    "        model.eval()\n",
    "    test(models, valid_loader, test_log)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plots of loss on validation set show smooth decrease of reconstruction and rho loss (for 256 latent dimensions and 50 epochs training). The model did not overfit on train data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(2, figsize=(12,9))\n",
    "ax[0].plot(np.array(test_log[\"256\"])[:,0])\n",
    "ax[0].set_title(\"Test reconstruction loss\")\n",
    "ax[1].plot(np.array(test_log[\"256\"])[:,1])\n",
    "ax[1].set_title(\"Test rho loss\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Checking the model outputs from different inputs. \n",
    "\n",
    "1. Reconstruction of batch of images from test set;\n",
    "2. Decoding identity matrix, where only one latent neuron is active;\n",
    "3. Counting number of hidden neurons with activations > 0.5. The model with 256 neurons has only few of them active at same time. Setting activations of those with < 0.5 to 0 to check what decoder can generate from those few."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data, _ = next(iter(test_loader))\n",
    "\n",
    "#(1.)\n",
    "output = models[\"256\"](data)\n",
    "to_plot = output.view(-1, 1, 28, 28).clamp(0, 1).data.numpy()\n",
    "\n",
    "#(2.)\n",
    "decoded = models[\"256\"].decode(torch.eye(256))\n",
    "dec_to_plot = ((decoded.view(-1, 1, 28, 28)+1)*0.5).clamp(0, 1).data.numpy()\n",
    "with torch.no_grad():\n",
    "    encoded = models[\"256\"].E(data.view(-1, 28*28))\n",
    "    \n",
    "    #(3.)\n",
    "    print(\"Number of neurons with activation > 0.5:\\n\", (encoded > 0.5).sum(1))\n",
    "    encoded[encoded < 0.5] = 0.    \n",
    "    decoded_f = models[\"256\"].decode(encoded)\n",
    "    f_to_plot = ((decoded_f.view(-1, 1, 28, 28)+1)*0.5).clamp(0, 1).data.numpy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plotting original and reconstructed images. The model makes a fairly decent reconstruction of input."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_mnist(data.data.numpy(), (5, 10))\n",
    "plot_mnist(to_plot, (5, 10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plot of images reconstructed from latent space with zeroed \"inactive\" neurons (activation < 0.5). Many of the numbers a recognizable, so sparse latent representation did learn useful features. The deactivated neurons mostly were responsible for background noise."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_mnist(f_to_plot, (5, 10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plot of decoded images from identity matrix. The images look very blurry, but even with only one neuron active in latent layer there can be seen some structure."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_mnist(dec_to_plot, (16, 16))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reconstruction of corrupted images.\n",
    "\n",
    "Training of SAE to remove noise."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add noise. \n",
    "\n",
    "def noise_pixels(x):\n",
    "    f = x + torch.randn_like(x)    \n",
    "    return f\n",
    "    \n",
    "def noise_batch(batch):\n",
    "    batch_z = batch.clone().detach()\n",
    "    for i in range(batch_z.shape[0]):\n",
    "        batch_z[i] = noise_pixels(batch_z[i])\n",
    "    return batch_z"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "models = {\"256\": Net(256)}\n",
    "rho = 0.05\n",
    "train_log = {k: [] for k in models}\n",
    "test_log = {k: [] for k in models}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for epoch in range(1, 51):\n",
    "    for model in models.values():\n",
    "        model.train()\n",
    "    train(epoch, models, train_log, add_noise=True)\n",
    "    for model in models.values():\n",
    "        model.eval()\n",
    "    test(models, valid_loader, test_log, add_noise=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(2, figsize=(12,9))\n",
    "ax[0].plot(np.array(test_log[\"256\"])[:,0])\n",
    "ax[0].set_title(\"Test reconstruction loss\")\n",
    "ax[1].plot(np.array(test_log[\"256\"])[:,1])\n",
    "ax[1].set_title(\"Test rho loss\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data, _ = next(iter(test_loader))\n",
    "inputs = noise_batch(data)\n",
    "output = models[\"256\"](inputs)\n",
    "to_plot = output.view(-1, 1, 28, 28).clamp(0, 1).data.numpy()\n",
    "\n",
    "decoded = models[\"256\"].decode(torch.eye(256))\n",
    "dec_to_plot = ((decoded.view(-1, 1, 28, 28)+1)*0.5).clamp(0, 1).data.numpy()\n",
    "\n",
    "with torch.no_grad():\n",
    "    encoded = models[\"256\"].E(inputs.view(-1, 28*28))    \n",
    "    print(\"Number of neurons with activation > 0.5:\\n\", (encoded > 0.5).sum(1))\n",
    "    encoded[encoded < 0.5] = 0.    \n",
    "    decoded_f = models[\"256\"].decode(encoded)\n",
    "    f_to_plot = ((decoded_f.view(-1, 1, 28, 28)+1)*0.5).clamp(0, 1).data.numpy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After 50 epochs of training model did quite a good job of reconstructing heavily noised images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_mnist(inputs.data.numpy(), (5, 10))\n",
    "plot_mnist(to_plot, (5, 10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And it performed even better with deactevated neurons (<0.5 set to 0) than when it was trained on clean images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_mnist(f_to_plot, (5, 10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Reconstructions from one active neuron also look more clear."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_mnist(dec_to_plot[:64], (8, 8))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Training on images cut in half."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cuts left part.\n",
    "\n",
    "def half_pixels(x):\n",
    "    f = x    \n",
    "    f[0,:,:14] = 0    \n",
    "    return f\n",
    "    \n",
    "def half_batch(batch):\n",
    "    batch_z = batch.clone().detach() \n",
    "    for i in range(batch_z.shape[0]):\n",
    "        batch_z[i] = half_pixels(batch_z[i])\n",
    "    return batch_z"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "models = {\"256\": Net(256)}\n",
    "rho = 0.05\n",
    "train_log = {k: [] for k in models}\n",
    "test_log = {k: [] for k in models}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for epoch in range(1, 51):\n",
    "    for model in models.values():\n",
    "        model.train()\n",
    "    train(epoch, models, train_log, half_image=True)\n",
    "    for model in models.values():\n",
    "        model.eval()\n",
    "    test(models, valid_loader, test_log, half_image=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(2, figsize=(12,9))\n",
    "ax[0].plot(np.array(test_log[\"256\"])[:,0])\n",
    "ax[0].set_title(\"Test reconstruction loss\")\n",
    "ax[1].plot(np.array(test_log[\"256\"])[:,1])\n",
    "ax[1].set_title(\"Test rho loss\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data, _ = next(iter(test_loader))\n",
    "inputs = half_batch(data)\n",
    "output = models[\"256\"](inputs)\n",
    "to_plot = output.view(-1, 1, 28, 28).clamp(0, 1).data.numpy()\n",
    "\n",
    "decoded = models[\"256\"].decode(torch.eye(256))\n",
    "dec_to_plot = ((decoded.view(-1, 1, 28, 28)+1)*0.5).clamp(0, 1).data.numpy()\n",
    "\n",
    "with torch.no_grad():\n",
    "    encoded = models[\"256\"].E(inputs.view(-1, 28*28))    \n",
    "    print(\"Number of neurons with activation > 0.5:\\n\", (encoded > 0.5).sum(1))\n",
    "    encoded[encoded < 0.5] = 0.    \n",
    "    decoded_f = models[\"256\"].decode(encoded)\n",
    "    f_to_plot = ((decoded_f.view(-1, 1, 28, 28)+1)*0.5).clamp(0, 1).data.numpy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plot of reconstructed half images from test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_mnist(inputs.data.numpy(), (5, 10))\n",
    "plot_mnist(to_plot, (5, 10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Again, looks like making task more difficult for SAE forces it to extract better features (than training on original images)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_mnist(f_to_plot, (5, 10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Although, reconstructions from identity matrix look worse in case of halved images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_mnist(dec_to_plot[:64], (8, 8))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
